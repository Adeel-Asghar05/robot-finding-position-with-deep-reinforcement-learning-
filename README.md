ğŸ¦¾ Robot Navigation with PyBullet + PPO

This project implements a custom robot navigation environment in PyBullet
 and trains it using Proximal Policy Optimization (PPO) from Stable-Baselines3
.

The robot (defined in ryres.urdf) learns to navigate towards random goal positions using reinforcement learning.

ğŸ“‚ Project Structure
â”œâ”€â”€ robot_env.py        # Main Python code (environment + training + testing)
â”œâ”€â”€ ryres.urdf          # Custom robot model (URDF format)
â”œâ”€â”€ meshe/              # STL mesh files for robot parts (used in URDF)
â””â”€â”€ README.md           # Project documentation

âš™ï¸ Features

âœ… Custom Gymnasium Environment (RobotEnv)

âœ… PyBullet physics simulation with gravity & collision

âœ… Discrete action space (Forward, Left, Right)

âœ… Reward shaping:

Distance progress reward

Heading alignment reward

Turning penalty/bonus

âœ… Custom callback (EpisodeTracker) for episode tracking

âœ… Training with PPO (Stable-Baselines3)

âœ… URDF robot model (ryres.urdf) with wheels, arms, and grippers

ğŸ¤– Robot Model (URDF)

The robot is defined in ryres.urdf, which contains:

Base link

Wheels (drive + support wheels)

Arms & grippers

Meshes (STL files in meshe/ directory)

PyBullet loads this URDF to simulate physics, dynamics, and collisions.

ğŸ“Š Training Rewards

The agent receives:

+10 Ã— distance progress

+0.5 Ã— cos(heading error) (alignment bonus)

+0.3 for purposeful turns (if they reduce heading error)

-0.045 per step (time penalty)

+100 for reaching the goal

-50 if maximum steps reached
